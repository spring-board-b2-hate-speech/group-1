{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  __Deep Learning Models__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned dataset\n",
    "df = pd.read_csv(r'C:\\group-1-main\\Model-Evaluvation\\cleaned_data.csv')\n",
    "df['tweet'] = df['tweet'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Handling Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df['tweet']\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the tokenizer on training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Address class imbalance using SMOTE on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_pad_res, y_train_pad_res = smote.fit_resample(X_train_pad, y_train)\n",
    "\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  __LSTM__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 137ms/step - accuracy: 0.5552 - loss: 1.5461 - val_accuracy: 0.7420 - val_loss: 0.6746\n",
      "Epoch 2/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 140ms/step - accuracy: 0.6849 - loss: 0.7347 - val_accuracy: 0.8082 - val_loss: 0.5626\n",
      "Epoch 3/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 137ms/step - accuracy: 0.7223 - loss: 0.6692 - val_accuracy: 0.7903 - val_loss: 0.6082\n",
      "Epoch 4/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 143ms/step - accuracy: 0.7520 - loss: 0.6125 - val_accuracy: 0.7826 - val_loss: 0.6527\n",
      "Epoch 5/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 143ms/step - accuracy: 0.7727 - loss: 0.5722 - val_accuracy: 0.7915 - val_loss: 0.6560\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8024 - loss: 0.5715\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step\n",
      "LSTM Model Accuracy: 0.808235764503479\n",
      "LSTM Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.21      0.57      0.31       282\n",
      "        Hate       0.97      0.82      0.89      3798\n",
      "   Offensive       0.75      0.84      0.79       874\n",
      "\n",
      "    accuracy                           0.81      4954\n",
      "   macro avg       0.64      0.74      0.66      4954\n",
      "weighted avg       0.89      0.81      0.84      4954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "lstm_model.add(SpatialDropout1D(0.2))  # Dropout layer to prevent overfitting\n",
    "lstm_model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01))))\n",
    "lstm_model.add(Dense(3, activation='softmax'))  # Assuming 3 classes\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model with early stopping\n",
    "lstm_model.fit(X_train_pad_res, y_train_pad_res, epochs=20, batch_size=64, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_pad, y_test)\n",
    "lstm_y_pred = lstm_model.predict(X_test_pad)\n",
    "lstm_y_pred_classes = np.argmax(lstm_y_pred, axis=1)\n",
    "lstm_report = classification_report(y_test, lstm_y_pred_classes, target_names=['Normal', 'Hate', 'Offensive'])\n",
    "\n",
    "print(\"LSTM Model Accuracy:\", lstm_accuracy)\n",
    "print(\"LSTM Model Classification Report:\\n\", lstm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  __CNN__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.6113 - loss: 0.8376 - val_accuracy: 0.8339 - val_loss: 0.4827\n",
      "Epoch 2/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.7740 - loss: 0.5429 - val_accuracy: 0.7868 - val_loss: 0.5988\n",
      "Epoch 3/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.8895 - loss: 0.3135 - val_accuracy: 0.8000 - val_loss: 0.5983\n",
      "Epoch 4/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.9497 - loss: 0.1628 - val_accuracy: 0.7778 - val_loss: 0.7348\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8313 - loss: 0.4881\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "CNN Model Accuracy: 0.833871603012085\n",
      "CNN Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.23      0.45      0.30       282\n",
      "        Hate       0.96      0.86      0.90      3798\n",
      "   Offensive       0.75      0.86      0.80       874\n",
      "\n",
      "    accuracy                           0.83      4954\n",
      "   macro avg       0.65      0.72      0.67      4954\n",
      "weighted avg       0.88      0.83      0.85      4954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(3, activation='softmax'))  # Assuming 3 classes\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model with early stopping\n",
    "cnn_model.fit(X_train_pad_res, y_train_pad_res, epochs=20, batch_size=64, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_pad, y_test)\n",
    "cnn_y_pred = cnn_model.predict(X_test_pad)\n",
    "cnn_y_pred_classes = np.argmax(cnn_y_pred, axis=1)\n",
    "cnn_report = classification_report(y_test, cnn_y_pred_classes, target_names=['Normal', 'Hate', 'Offensive'])\n",
    "\n",
    "print(\"CNN Model Accuracy:\", cnn_accuracy)\n",
    "print(\"CNN Model Classification Report:\\n\", cnn_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  __Bidirectional LSTM__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 134ms/step - accuracy: 0.5566 - loss: 1.5559 - val_accuracy: 0.7951 - val_loss: 0.5978\n",
      "Epoch 2/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 126ms/step - accuracy: 0.6838 - loss: 0.7385 - val_accuracy: 0.8066 - val_loss: 0.5818\n",
      "Epoch 3/20\n",
      "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 128ms/step - accuracy: 0.7224 - loss: 0.6699 - val_accuracy: 0.7840 - val_loss: 0.6472\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7918 - loss: 0.6058\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step\n",
      "Bidirectional LSTM Model Accuracy: 0.795115053653717\n",
      "Bidirectional LSTM Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.19      0.50      0.27       282\n",
      "        Hate       0.97      0.79      0.87      3798\n",
      "   Offensive       0.71      0.91      0.80       874\n",
      "\n",
      "    accuracy                           0.80      4954\n",
      "   macro avg       0.62      0.73      0.65      4954\n",
      "weighted avg       0.88      0.80      0.83      4954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Bidirectional LSTM model\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "bi_lstm_model.add(SpatialDropout1D(0.2))  # Dropout layer to prevent overfitting\n",
    "bi_lstm_model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01))))\n",
    "bi_lstm_model.add(Dense(3, activation='softmax'))  # Assuming 3 classes\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "bi_lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the Bidirectional LSTM model with early stopping\n",
    "bi_lstm_model.fit(X_train_pad_res, y_train_pad_res, epochs=20, batch_size=64, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate Bidirectional LSTM model\n",
    "bi_lstm_loss, bi_lstm_accuracy = bi_lstm_model.evaluate(X_test_pad, y_test)\n",
    "bi_lstm_y_pred = bi_lstm_model.predict(X_test_pad)\n",
    "bi_lstm_y_pred_classes = np.argmax(bi_lstm_y_pred, axis=1)\n",
    "bi_lstm_report = classification_report(y_test, bi_lstm_y_pred_classes, target_names=['Normal', 'Hate', 'Offensive'])\n",
    "\n",
    "print(\"Bidirectional LSTM Model Accuracy:\", bi_lstm_accuracy)\n",
    "print(\"Bidirectional LSTM Model Classification Report:\\n\", bi_lstm_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
