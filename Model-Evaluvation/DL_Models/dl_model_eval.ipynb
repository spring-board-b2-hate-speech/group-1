{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  __Deep Learning Models__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Input, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    20608\n",
      "1     4158\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\group-1-main\\\\Model-Evaluvation\\\\cleaned_data.csv')\n",
    "\n",
    "# Ensure all entries in the text column are strings\n",
    "df['tweet'] = df['tweet'].astype(str).fillna('')\n",
    "\n",
    "# Filter the dataset to include only hate speech (0), offensive language (1), and non-hate speech (2)\n",
    "df = df[df['class'].isin([0, 1, 2])]\n",
    "\n",
    "# Encode the labels (0 and 1 for hate speech and offensive language, 2 for non-hate speech)\n",
    "label_mapping = {0: 0, 1: 0, 2: 1}\n",
    "df['label'] = df['class'].map(label_mapping)\n",
    "label_distribution = df['label'].value_counts()\n",
    "print(label_distribution)\n",
    "\n",
    "# Split the data\n",
    "X = df['tweet'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure all training and test data are strings\n",
    "X_train = [str(text) for text in X_train]\n",
    "X_test = [str(text) for text in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Padding the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000)  # Adjust num_words as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = 100  # Adjust max_length as needed\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (33056, 100)\n",
      "Shape of test data: (4954, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath, word_index, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_filepath = 'C:\\\\group-1-main\\\\Model-Evaluvation\\\\DL_Models\\\\glove.6B.100d.txt'  # Update the path to your GloVe file\n",
    "embedding_matrix = load_glove_embeddings(glove_filepath, tokenizer.word_index, embedding_dim)\n",
    "\n",
    "print(\"Shape of training data:\", X_train_pad_res.shape)\n",
    "print(\"Shape of test data:\", X_test_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Layer\n",
    "    -This cell defines a custom layer to compute the mean of the input tensor along the specified axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer to wrap tf.reduce_mean\n",
    "class ReduceMeanLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  __LSTM__ model with Attention Model\n",
    "    -LSTM model with an Attention mechanism. The model includes an embedding layer, LSTM layer, attention layer, dense layer, and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model with Attention\n",
    "def create_lstm_attention_model():\n",
    "    inputs = Input(shape=(max_length,))\n",
    "    embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                                output_dim=embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)(inputs)\n",
    "    lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
    "    attention_layer = Attention()([lstm_layer, lstm_layer])\n",
    "    attention_output = ReduceMeanLayer()(attention_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(attention_output)\n",
    "    dropout_layer = Dropout(0.5)(dense_layer)\n",
    "    outputs = Dense(2, activation='softmax')(dropout_layer)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  __CNN__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_length,\n",
    "                        trainable=False))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  __Bidirectional LSTM__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bidirectional LSTM model\n",
    "def create_bidirectional_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_length,\n",
    "                        trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, activation='tanh', recurrent_activation='sigmoid')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 - 32s - 64ms/step - accuracy: 0.8879 - loss: 0.2472 - val_accuracy: 0.9140 - val_loss: 0.1872\n",
      "Epoch 2/20\n",
      "496/496 - 28s - 57ms/step - accuracy: 0.9162 - loss: 0.1694 - val_accuracy: 0.9187 - val_loss: 0.1731\n",
      "Epoch 3/20\n",
      "496/496 - 29s - 58ms/step - accuracy: 0.9231 - loss: 0.1574 - val_accuracy: 0.9266 - val_loss: 0.1676\n",
      "Epoch 4/20\n",
      "496/496 - 29s - 58ms/step - accuracy: 0.9281 - loss: 0.1483 - val_accuracy: 0.9263 - val_loss: 0.1643\n",
      "Epoch 5/20\n",
      "496/496 - 28s - 57ms/step - accuracy: 0.9330 - loss: 0.1394 - val_accuracy: 0.9230 - val_loss: 0.1772\n",
      "Epoch 6/20\n",
      "496/496 - 28s - 56ms/step - accuracy: 0.9377 - loss: 0.1333 - val_accuracy: 0.9251 - val_loss: 0.1690\n",
      "Epoch 7/20\n",
      "496/496 - 28s - 56ms/step - accuracy: 0.9442 - loss: 0.1236 - val_accuracy: 0.9326 - val_loss: 0.1653\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate LSTM with Attention model\n",
    "lstm_attention_model = create_lstm_attention_model()\n",
    "history_lstm_attention = lstm_attention_model.fit(X_train_pad, y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "y_pred_lstm_attention = np.argmax(lstm_attention_model.predict(X_test_pad), axis=1)\n",
    "accuracy_lstm_attention = accuracy_score(y_test, y_pred_lstm_attention)\n",
    "f1_lstm_attention = f1_score(y_test, y_pred_lstm_attention, average='weighted')\n",
    "report_lstm_attention = classification_report(y_test, y_pred_lstm_attention, target_names=['Hate Speech', 'Non-Hate Speech'])\n",
    "\n",
    "# Save LSTM with Attention model\n",
    "lstm_attention_model.save('lstm_attention_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 - 41s - 82ms/step - accuracy: 0.9017 - loss: 0.2173 - val_accuracy: 0.9165 - val_loss: 0.1778\n",
      "Epoch 2/20\n",
      "496/496 - 37s - 74ms/step - accuracy: 0.9264 - loss: 0.1583 - val_accuracy: 0.9238 - val_loss: 0.1665\n",
      "Epoch 3/20\n",
      "496/496 - 36s - 72ms/step - accuracy: 0.9346 - loss: 0.1445 - val_accuracy: 0.9246 - val_loss: 0.1657\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Bidirectional LSTM model\n",
    "bidirectional_lstm_model = create_bidirectional_lstm_model()\n",
    "history_bidirectional_lstm = bidirectional_lstm_model.fit(X_train_pad, y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "y_pred_bidirectional_lstm = np.argmax(bidirectional_lstm_model.predict(X_test_pad), axis=1)\n",
    "accuracy_bidirectional_lstm = accuracy_score(y_test, y_pred_bidirectional_lstm)\n",
    "f1_bidirectional_lstm = f1_score(y_test, y_pred_bidirectional_lstm, average='weighted')\n",
    "report_bidirectional_lstm = classification_report(y_test, y_pred_bidirectional_lstm, target_names=['Hate Speech', 'Non-Hate Speech'])\n",
    "\n",
    "# Save Bidirectional LSTM model\n",
    "bidirectional_lstm_model.save('bidirectional_lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 - 6s - 12ms/step - accuracy: 0.8930 - loss: 0.2396 - val_accuracy: 0.9172 - val_loss: 0.1772\n",
      "Epoch 2/20\n",
      "496/496 - 5s - 10ms/step - accuracy: 0.9282 - loss: 0.1597 - val_accuracy: 0.9215 - val_loss: 0.1690\n",
      "Epoch 3/20\n",
      "496/496 - 5s - 9ms/step - accuracy: 0.9420 - loss: 0.1369 - val_accuracy: 0.9208 - val_loss: 0.1758\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate CNN model\n",
    "cnn_model = create_cnn_model()\n",
    "history_cnn = cnn_model.fit(X_train_pad, y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "y_pred_cnn = np.argmax(cnn_model.predict(X_test_pad), axis=1)\n",
    "accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
    "f1_cnn = f1_score(y_test, y_pred_cnn, average='weighted')\n",
    "report_cnn = classification_report(y_test, y_pred_cnn, target_names=['Hate Speech', 'Non-Hate Speech'])\n",
    "\n",
    "# Save CNN model\n",
    "cnn_model.save('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models and Saving the Best One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  F1 Score\n",
      "0  LSTM with Attention  0.923496  0.925898\n",
      "1   Bidirectional LSTM  0.916027  0.917746\n",
      "2                  CNN  0.921074  0.920877\n",
      "The best model is LSTM with Attention with an F1 Score of 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Compare model performances\n",
    "model_performance = {\n",
    "    'Model': ['LSTM with Attention', 'Bidirectional LSTM', 'CNN'],\n",
    "    'Accuracy': [accuracy_lstm_attention, accuracy_bidirectional_lstm, accuracy_cnn],\n",
    "    'F1 Score': [f1_lstm_attention, f1_bidirectional_lstm, f1_cnn]\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(model_performance)\n",
    "\n",
    "# Display the performance of each model\n",
    "print(performance_df)\n",
    "\n",
    "# Determine the best model based on F1 Score\n",
    "best_model_index = performance_df['F1 Score'].idxmax()\n",
    "best_model_name = performance_df.iloc[best_model_index]['Model']\n",
    "\n",
    "# Save the best model\n",
    "if best_model_name == 'LSTM with Attention':\n",
    "    best_model = lstm_attention_model\n",
    "elif best_model_name == 'Bidirectional LSTM':\n",
    "    best_model = bidirectional_lstm_model\n",
    "else:\n",
    "    best_model = cnn_model\n",
    "\n",
    "best_model.save('best_model.h5')\n",
    "\n",
    "print(f\"The best model is {best_model_name} with an F1 Score of {performance_df.iloc[best_model_index]['F1 Score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
