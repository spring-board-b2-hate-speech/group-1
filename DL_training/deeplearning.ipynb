{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, GRU, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (19826, 100)\n",
      "Shape of test data: (4957, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('final.csv')\n",
    "text_column = 'tweet'  # Adjust if the column name is different\n",
    "\n",
    "# Ensure all entries in the text column are strings\n",
    "df[text_column] = df[text_column].astype(str).fillna('')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['class'])\n",
    "\n",
    "# Split the data\n",
    "X = df[text_column].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure all training and test data are strings\n",
    "X_train = [str(text) for text in X_train]\n",
    "X_test = [str(text) for text in X_test]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000)  # Adjust num_words as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = 100  # Adjust max_length as needed\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "print(\"Shape of training data:\", X_train_pad.shape)\n",
    "print(\"Shape of test data:\", X_test_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define Bidirectional LSTM model\n",
    "def create_bidirectional_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define CNN model\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chish\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 - 42s - 85ms/step - accuracy: 0.8578 - loss: 0.3885 - val_accuracy: 0.9090 - val_loss: 0.2748\n",
      "Epoch 2/10\n",
      "496/496 - 43s - 88ms/step - accuracy: 0.9262 - loss: 0.2134 - val_accuracy: 0.8996 - val_loss: 0.2849\n",
      "Epoch 3/10\n",
      "496/496 - 42s - 85ms/step - accuracy: 0.9510 - loss: 0.1416 - val_accuracy: 0.8908 - val_loss: 0.3565\n",
      "Epoch 4/10\n",
      "496/496 - 37s - 75ms/step - accuracy: 0.9656 - loss: 0.0982 - val_accuracy: 0.8782 - val_loss: 0.4494\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step\n",
      "Epoch 1/10\n",
      "496/496 - 54s - 108ms/step - accuracy: 0.8586 - loss: 0.3941 - val_accuracy: 0.9060 - val_loss: 0.2725\n",
      "Epoch 2/10\n",
      "496/496 - 51s - 102ms/step - accuracy: 0.9227 - loss: 0.2202 - val_accuracy: 0.8966 - val_loss: 0.2897\n",
      "Epoch 3/10\n",
      "496/496 - 51s - 103ms/step - accuracy: 0.9491 - loss: 0.1467 - val_accuracy: 0.8853 - val_loss: 0.3446\n",
      "Epoch 4/10\n",
      "496/496 - 50s - 101ms/step - accuracy: 0.9656 - loss: 0.1008 - val_accuracy: 0.8822 - val_loss: 0.4051\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step\n",
      "Epoch 1/10\n",
      "496/496 - 15s - 30ms/step - accuracy: 0.8687 - loss: 0.3641 - val_accuracy: 0.9057 - val_loss: 0.2633\n",
      "Epoch 2/10\n",
      "496/496 - 14s - 28ms/step - accuracy: 0.9298 - loss: 0.1983 - val_accuracy: 0.8994 - val_loss: 0.2895\n",
      "Epoch 3/10\n",
      "496/496 - 14s - 29ms/step - accuracy: 0.9650 - loss: 0.1031 - val_accuracy: 0.8966 - val_loss: 0.3689\n",
      "Epoch 4/10\n",
      "496/496 - 14s - 28ms/step - accuracy: 0.9820 - loss: 0.0543 - val_accuracy: 0.8893 - val_loss: 0.4139\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "F1 Scores: {'LSTM': 0.8750802819172352, 'Bidirectional LSTM': 0.8682262856869913, 'CNN': 0.8881515796526138}\n"
     ]
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1\n",
    "\n",
    "# Create models\n",
    "lstm_model = create_lstm_model()\n",
    "bidirectional_lstm_model = create_bidirectional_lstm_model()\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "# Train and evaluate models\n",
    "f1_scores = {}\n",
    "f1_scores['LSTM'] = train_and_evaluate_model(lstm_model, X_train_pad, y_train, X_test_pad, y_test)\n",
    "f1_scores['Bidirectional LSTM'] = train_and_evaluate_model(bidirectional_lstm_model, X_train_pad, y_train, X_test_pad, y_test)\n",
    "f1_scores['CNN'] = train_and_evaluate_model(cnn_model, X_train_pad, y_train, X_test_pad, y_test)\n",
    "\n",
    "print(\"F1 Scores:\", f1_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is CNN and it has been saved as best_deep_learning_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'LSTM':\n",
    "    best_model = lstm_model\n",
    "elif best_model_name == 'Bidirectional LSTM':\n",
    "    best_model = bidirectional_lstm_model\n",
    "elif best_model_name == 'CNN':\n",
    "    best_model = cnn_model\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_deep_learning_model.h5')\n",
    "print(f'The best model is {best_model_name} and it has been saved as best_deep_learning_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m775/775\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step\n",
      "                                                   tweet  predictions\n",
      "0      woman complain cleaning house man always take ...            2\n",
      "1                  boy coldtyga bad cuffin hoe 1st place            1\n",
      "2         friend ever fuck bitch start cry confused shit            1\n",
      "3                                       look like tranny            2\n",
      "4            shit hear might true might faker bitch told            1\n",
      "...                                                  ...          ...\n",
      "24778  yous muthafin lie right tl trash mine bible sc...            2\n",
      "24779    gone broke wrong heart baby drove redneck crazy            2\n",
      "24780        young buck wanna eat nigguh like fuckin dis            1\n",
      "24781                  youu got wild bitches tellin lies            1\n",
      "24782  ruffled ntac eileen dahlia beautiful color com...            2\n",
      "\n",
      "[24783 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = load_model('best_deep_learning_model.h5')\n",
    "\n",
    "# Load the final dataset\n",
    "final_df = pd.read_csv('final.csv')\n",
    "final_df[text_column] = final_df[text_column].astype(str).fillna('')\n",
    "X_new = final_df[text_column]\n",
    "\n",
    "# Tokenize and pad the new data\n",
    "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
    "X_new_pad = pad_sequences(X_new_seq, maxlen=max_length)\n",
    "\n",
    "# Predict using the best model\n",
    "new_predictions = best_model.predict(X_new_pad)\n",
    "new_predictions_labels = np.argmax(new_predictions, axis=1)\n",
    "\n",
    "# Map predictions to labels\n",
    "new_predictions_labels = label_encoder.inverse_transform(new_predictions_labels)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "final_df['predictions'] = new_predictions_labels\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "final_df.to_csv('final_with_deep_learning_predictions.csv', index=False)\n",
    "\n",
    "# Print the predictions\n",
    "print(final_df[['tweet', 'predictions']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
