{"cells":[{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import re\n","import string\n","import joblib\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class ReduceMeanLayer(tf.keras.layers.Layer):\n","    def call(self, inputs):\n","        return tf.reduce_mean(inputs, axis=1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#load tokenizer and model\n","tokenizer_save_path = 'tokenizer.joblib'\n","model_save_path = 'best_lstm_model.keras'"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["tokenizer = joblib.load(tokenizer_save_path)\n","model = load_model(model_save_path, custom_objects={'ReduceMeanLayer': ReduceMeanLayer})"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Function to preprocess the input text\n","def preprocess_text(text, tokenizer, max_length=100):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\b\\d+\\b', '', text)\n","    text = ''.join([char for char in text if char not in string.punctuation])\n","    text = ' '.join([WordNetLemmatizer().lemmatize(word) for word in text.split() if word not in stopwords.words('english')])\n","    \n","    seq = tokenizer.texts_to_sequences([text])\n","    padded_seq = pad_sequences(seq, maxlen=max_length)\n","    \n","    return padded_seq"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#function to make predictions\n","def predict_text(text):\n","    processed_text = preprocess_text(text, tokenizer)\n","    prediction = model.predict(processed_text)\n","    predicted_label = np.argmax(prediction, axis=1)[0]\n","    return 'Non Hate Speech' if predicted_label == 1 else 'Hate Speech'"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input text 1: he moviwatche is literraly shit, dont watch this fckkngg%^%^% movie\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","The input text is classified as: Hate Speech\n","\n","Input text 2: i dont like that black nigga\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","The input text is classified as: Hate Speech\n","\n","Input text 3: movie for me was below average , although some might like it\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","The input text is classified as: Non Hate Speech\n","\n","Input text 4: nice , i love it\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","The input text is classified as: Non Hate Speech\n","\n","Input text 5: this movie is very gooooood\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","The input text is classified as: Non Hate Speech\n","\n"]}],"source":["for i in range(0, 5):\n","    input_text = str(input(\"Enter the text to predict: \"))\n","    print(f\"Input text {i + 1}: {input_text}\")\n","    prediction = predict_text(input_text)\n","    print(f\"The input text is classified as: {prediction}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
