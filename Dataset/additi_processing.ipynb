{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_labeled_data.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  rt mayasolov woman shouldnt complain clean hou...  \n",
      "1  rt mleew17 boy dat coldtyga dwn bad cuffin dat...  \n",
      "2  rt urkindofbrand dawg rt 80sbaby4lif ever fuck...  \n",
      "3             rt cganderson vivabas look like tranni  \n",
      "4  rt shenikarobert shit hear might true might fa...  \n"
     ]
    }
   ],
   "source": [
    "# Further preprocessing: remove stop words, tokenize, and apply stemming/lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    # Apply lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column\n",
    "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the further cleaned dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final cleaned dataset saved to final_data.csv\n"
     ]
    }
   ],
   "source": [
    "fp = 'further_cleaned_data.csv'\n",
    "df.to_csv(fp, index=False)\n",
    "\n",
    "print(f\"final cleaned dataset saved to {fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
